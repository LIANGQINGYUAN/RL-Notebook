{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2018 The TF-Agents Authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Started\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/5_replay_buffers_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/5_replay_buffers_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't installed tf-agents or gym yet, run:\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    pass\n",
    "!pip install tf-agents-nightly\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents import specs\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.replay_buffers import py_uniform_replay_buffer\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简介\n",
    "\n",
    "在环境中执行策略时，强化学习算法使用重播缓冲区来存储经验轨迹。在训练过程中，重播缓冲区被用于查询一个轨迹子集(一个连续子集或一个样本)来“重播”代理的经验。\n",
    "\n",
    "在这个colab中，我们研究了两种类型的重播缓冲区:python支持的和tensorflow支持的，它们共享一个公共API。在下面的小节中，我们将介绍API、每个缓冲区实现以及如何在数据收集培训期间使用它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重播缓冲区的API\n",
    "重播缓冲类有如下定义和方法:\n",
    "\n",
    "```python\n",
    "class ReplayBuffer(tf.Module):\n",
    "  \"\"\"Abstract base class for TF-Agents replay buffer.\"\"\"\n",
    "\n",
    "  def __init__(self, data_spec, capacity):\n",
    "    \"\"\"Initializes the replay buffer.\n",
    "\n",
    "    Args:\n",
    "      data_spec: A spec or a list/tuple/nest of specs describing\n",
    "        a single item that can be stored in this buffer\n",
    "      capacity: number of elements that the replay buffer can hold.\n",
    "    \"\"\"\n",
    "\n",
    "  @property\n",
    "  def data_spec(self):\n",
    "    \"\"\"Returns the spec for items in the replay buffer.\"\"\"\n",
    "\n",
    "  @property\n",
    "  def capacity(self):\n",
    "    \"\"\"Returns the capacity of the replay buffer.\"\"\"\n",
    "\n",
    "  def add_batch(self, items):\n",
    "    \"\"\"Adds a batch of items to the replay buffer.\"\"\"\n",
    "\n",
    "  def get_next(self,\n",
    "               sample_batch_size=None,\n",
    "               num_steps=None,\n",
    "               time_stacked=True):\n",
    "    \"\"\"Returns an item or batch of items from the buffer.\"\"\"\n",
    "\n",
    "  def as_dataset(self,\n",
    "                 sample_batch_size=None,\n",
    "                 num_steps=None,\n",
    "                 num_parallel_calls=None):\n",
    "    \"\"\"Creates and returns a dataset that returns entries from the buffer.\"\"\"\n",
    "\n",
    "\n",
    "  def gather_all(self):\n",
    "    \"\"\"Returns all the items in buffer.\"\"\"\n",
    "    return self._gather_all()\n",
    "\n",
    "  def clear(self):\n",
    "    \"\"\"Resets the contents of replay buffer\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "请注意，当重播缓冲区对象初始化时，需要将 `data_spec`中的元素存储，该规格对应于将被添加到缓冲区的路径元素的 `TensorSpec` 。这个规范通常是通过查看代理的`agent.collect_data_spec`获得的，它定义了训练时代理期望的形状、类型和结构(稍后将详细介绍)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFUniformReplayBuffer\n",
    "\n",
    "`TFUniformReplayBuffer` 是一个TF-Agents中最常使用的重播缓冲区，所以这个教程也使用它。在`TFUniformReplayBuffer`中，后备缓冲区存储是由tensorflow变量完成的，因此它是计算图的一部分。\n",
    "\n",
    "缓冲区存储了一批元素，并且每个批处理段有一个最大容量的“max_length”元素。因此，总的缓冲区容量是' batch_size ' 乘以 ' max_length '个元素。存储在缓冲区中的元素必须具有一个匹配的数据规范。当回放缓冲区用于数据收集时，该规范是代理的收集数据规范。\n",
    "\n",
    "## 创建一个缓冲区:\n",
    "要创建一个`TFUniformReplayBuffer` 我们传入：\n",
    "1. 缓冲区将存储的数据元素的规范\n",
    "2. 与缓冲区的批大小对应的`batch size`  \n",
    "3. 每个批处理段的`max_length`元素数\n",
    "\n",
    "下面是创建一个`TFUniformReplayBuffer`的示例，`batch_size` 为32， `max_length` 为1000。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spec =  (\n",
    "        tf.TensorSpec([3], tf.float32, 'action'),\n",
    "        (\n",
    "            tf.TensorSpec([5], tf.float32, 'lidar'),\n",
    "            tf.TensorSpec([3, 2], tf.float32, 'camera')\n",
    "        )\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "max_length = 1000\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 写入缓冲区\n",
    "向重播缓冲区中加入元素需要使用`add_batch(items)` 方法，其中`items`是一个tensor的list/tuple/nest，表示要添加到缓冲区的批处理项。`items`的每个元素都必须有一个与`batch_size`相等的外部维度，其余维度必须符合该项的数据规范(与传递给回放缓冲区构造函数的数据规范相同)。\n",
    "\n",
    "下面是添加一个batch的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = tf.constant(1 * np.ones(\n",
    "    data_spec[0].shape.as_list(), dtype=np.float32))\n",
    "lidar = tf.constant(\n",
    "    2 * np.ones(data_spec[1][0].shape.as_list(), dtype=np.float32))\n",
    "camera = tf.constant(\n",
    "    3 * np.ones(data_spec[1][1].shape.as_list(), dtype=np.float32))\n",
    "  \n",
    "values = (action, (lidar, camera))\n",
    "values_batched = tf.nest.map_structure(lambda t: tf.stack([t] * batch_size),\n",
    "                                       values)\n",
    "  \n",
    "replay_buffer.add_batch(values_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取缓冲区内容\n",
    "\n",
    "有3种方法可以从`TFUniformReplayBuffer`中读取数据：\n",
    "\n",
    "1. `get_next()` ：从缓冲区返回一个样本。可以通过此方法的参数指定返回的样例批处理大小和时间步长。\n",
    "2. `as_dataset()`：将重播缓冲区作为' tf.data.Dataset '返回。然后可以创建一个数据集迭代器，并遍历缓冲区中的项的样本。\n",
    "3. `gather_all()` ：以一个张量的形式返回缓冲区中的所有项，该张量的形状为‘[batch, time, data_spec]’。\n",
    "\n",
    "下面是如何使用这些方法从重播缓冲区读取的例子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterator trajectories:\n",
      "[(TensorShape([4, 2, 3]), (TensorShape([4, 2, 5]), TensorShape([4, 2, 3, 2]))), (TensorShape([4, 2, 3]), (TensorShape([4, 2, 5]), TensorShape([4, 2, 3, 2]))), (TensorShape([4, 2, 3]), (TensorShape([4, 2, 5]), TensorShape([4, 2, 3, 2])))]\n",
      "Trajectories from gather all:\n",
      "(TensorShape([32, 6, 3]), (TensorShape([32, 6, 5]), TensorShape([32, 6, 3, 2])))\n"
     ]
    }
   ],
   "source": [
    "# add more items to the buffer before reading\n",
    "for _ in range(5):\n",
    "    replay_buffer.add_batch(values_batched)\n",
    "\n",
    "# Get one sample from the replay buffer with batch size 10 and 1 timestep:\n",
    "\n",
    "sample = replay_buffer.get_next(sample_batch_size=10, num_steps=1)\n",
    "\n",
    "# Convert the replay buffer to a tf.data.Dataset and iterate through it\n",
    "dataset = replay_buffer.as_dataset(sample_batch_size=4, num_steps=2)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "print(\"Iterator trajectories:\")\n",
    "trajectories = []\n",
    "for _ in range(3):\n",
    "    t, _ = next(iterator)\n",
    "    trajectories.append(t)\n",
    "\n",
    "print(tf.nest.map_structure(lambda t: t.shape, trajectories))\n",
    "\n",
    "# Read all elements in the replay buffer:\n",
    "trajectories = replay_buffer.gather_all()\n",
    "\n",
    "print(\"Trajectories from gather all:\")\n",
    "print(tf.nest.map_structure(lambda t: t.shape, trajectories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyUniformReplayBuffer\n",
    "`PyUniformReplayBuffer` 和`TFUniformReplayBuffer` 有着相同的功能，但并不适用tf的变量，而是直接将数据存储为numpy的数组。此缓冲区可用于图形外数据收集。在numpy中拥有备份存储可以使某些应用程序更容易地执行数据操作(例如为更新优先级建立索引)，而无需使用Tensorflow变量。但是，这个实现没有Tensorflow图形优化的好处。\n",
    "\n",
    "下面是一个实例，从代理的策略轨迹规范中实例化一个“PyUniformReplayBuffer”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_capacity = 1000*32 # same capacity as the TFUniformReplayBuffer\n",
    "\n",
    "py_replay_buffer = py_uniform_replay_buffer.PyUniformReplayBuffer(\n",
    "    capacity=replay_buffer_capacity,\n",
    "    data_spec=tensor_spec.to_nest_array_spec(data_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在训练的过程中使用重播缓冲区\n",
    "\n",
    "现在我们知道了如何创建一个重播缓冲区，向它写入数据并从中读取数据，我们可以使用它来存储训练代理时的轨迹。\n",
    "\n",
    "## 数据收集\n",
    "首先，让我们看看如何在数据收集期间使用重播缓冲区。\n",
    "\n",
    "在TF-Agents中，我们使用`Driver`(有关更多细节，请参阅`Driver`教程)来收集环境中的经验。要使用`Driver`，我们指定一个“观察者”`Observer` ，它是一个函数，当`Driver` 接收到一个轨迹时，它将执行这个函数。\n",
    "\n",
    "因此，为了将轨迹元素添加到回放缓冲区，我们添加了一个用`add_batch(items)`的观察者来在回放缓冲区中添加(batch)项。\n",
    "\n",
    "\n",
    "下面是一个`TFUniformReplayBuffer`的例子。我们首先创建一个环境、一个网络和一个代理。然后我们创建一个TFUniformReplayBuffer。请注意，重播缓冲区中轨迹元素的规格等于代理的收集数据规格。然后，我们将其`add_batch`方法设置为driver的观察者，在我们的训练期间，该方法将进行数据收集:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    tf_env.time_step_spec().observation,\n",
    "    tf_env.action_spec(),\n",
    "    fc_layer_params=(100,))\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=tf.compat.v1.train.AdamOptimizer(0.001))\n",
    "\n",
    "replay_buffer_capacity = 1000\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)\n",
    "\n",
    "# Add an observer that adds to the replay buffer:\n",
    "replay_observer = [replay_buffer.add_batch]\n",
    "\n",
    "collect_steps_per_iteration = 10\n",
    "collect_op = dynamic_step_driver.DynamicStepDriver(\n",
    "  tf_env,\n",
    "  agent.collect_policy,\n",
    "  observers=replay_observer,\n",
    "  num_steps=collect_steps_per_iteration).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在训练的步骤中读取数据\n",
    "\n",
    "将轨迹元素添加到重播缓冲区后，我们可以从重播缓冲区中读取成批的轨迹作为训练步长的输入数据。\n",
    "\n",
    "下面是一个如何在训练循环中从重播缓冲区训练轨迹的例子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the replay buffer as a Dataset,\n",
    "# read batches of 4 elements, each with 2 timesteps:\n",
    "dataset = replay_buffer.as_dataset(sample_batch_size=4, num_steps=2)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "num_train_steps = 10\n",
    "\n",
    "for _ in range(num_train_steps):\n",
    "    trajectories, _ = next(iterator)\n",
    "    loss = agent.train(experience=trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
