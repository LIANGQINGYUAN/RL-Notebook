{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2018 The TF-Agents Authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Started\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/4_drivers_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/4_drivers_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: If you haven't installed tf-agents or gym yet, run:\n",
    "try:\n",
    "    %%tensorflow_version 2.x\n",
    "except:\n",
    "    pass\n",
    "!pip install tf-agents-nightly\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简介\n",
    "\n",
    "强化学习的基本模式是在一个环境中执行一个策略，并迭代一定数量的步数或事件。例如，在数据收集、评估和生成代理的视频期间都包含在其中。\n",
    "\n",
    "虽然用python编写相对简单，但在TensorFlow中编写和调试要复杂得多，因为它涉及到 `tf.while` 循环, `tf.cond` 和 `tf.control_dependencies`. 因此，我们将run循环的概念抽象为一个名为 `driver`的类，并在Python和TensorFlow中提供了经过良好测试的实现。\n",
    "\n",
    "此外，driver在每个步骤中遇到的数据被保存在一个名为轨迹（Trajectory）的命名元组中，并广播给一组观察者，如重播缓冲区和度量。这些数据包括来自环境的观察，策略建议的动作，获得的奖励，当前和下一步的类型等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Drivers\n",
    "\n",
    " `PyDriver` 类接受一个python环境、一个python策略和一个观察者列表，以便在每一步更新。主要的方法是`run()`, 它使用策略中的操作逐步处理环境，直到至少满足下列终止条件之一:步骤的数量达到 `max_steps`或剧集的数量达到 `max_episodes`。\n",
    "\n",
    "具体实施情况大致如下:\n",
    "\n",
    "```python\n",
    "class PyDriver(object):\n",
    "\n",
    "  def __init__(self, env, policy, observers, max_steps=1, max_episodes=1):\n",
    "    self._env = env\n",
    "    self._policy = policy\n",
    "    self._observers = observers or []\n",
    "    self._max_steps = max_steps or np.inf\n",
    "    self._max_episodes = max_episodes or np.inf\n",
    "\n",
    "  def run(self, time_step, policy_state=()):\n",
    "    num_steps = 0\n",
    "    num_episodes = 0\n",
    "    while num_steps < self._max_steps and num_episodes < self._max_episodes:\n",
    "\n",
    "      # Compute an action using the policy for the given time_step\n",
    "      action_step = self._policy.action(time_step, policy_state)\n",
    "\n",
    "      # Apply the action to the environment and get the next step\n",
    "      next_time_step = self._env.step(action_step.action)\n",
    "\n",
    "      # Package information into a trajectory\n",
    "      traj = trajectory.Trajectory(\n",
    "         time_step.step_type,\n",
    "         time_step.observation,\n",
    "         action_step.action,\n",
    "         action_step.info,\n",
    "         next_time_step.step_type,\n",
    "         next_time_step.reward,\n",
    "         next_time_step.discount)\n",
    "\n",
    "      for observer in self._observers:\n",
    "        observer(traj)\n",
    "\n",
    "      # Update statistics to check termination\n",
    "      num_episodes += np.sum(traj.is_last())\n",
    "      num_steps += np.sum(~traj.is_boundary())\n",
    "\n",
    "      time_step = next_time_step\n",
    "      policy_state = action_step.state\n",
    "\n",
    "    return time_step, policy_state\n",
    "\n",
    "```\n",
    "\n",
    "现在，让我们运行一个在CartPole环境上运行随机策略的示例，将结果保存到回放缓冲区并计算一些度量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer:\n",
      "Trajectory(step_type=array(0, dtype=int32), observation=array([-0.02310036, -0.02402413,  0.02206977,  0.04428826], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.02358084, -0.2194555 ,  0.02295554,  0.3438519 ], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.02796995, -0.0246675 ,  0.02983258,  0.05849522], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.0284633 , -0.2202042 ,  0.03100248,  0.36043924], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.03286739, -0.41575283,  0.03821126,  0.6627345 ], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.04118244, -0.611385  ,  0.05146595,  0.9671999 ], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.05341014, -0.8071589 ,  0.07080995,  1.2755963 ], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.06955332, -1.0031089 ,  0.09632188,  1.5895854 ], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.08961549, -1.1992339 ,  0.12811358,  1.9106841 ], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.11360017, -1.3954836 ,  0.16632727,  2.2402086 ], dtype=float32), action=array(0), policy_info=(), next_step_type=array(2, dtype=int32), reward=array(1., dtype=float32), discount=array(0., dtype=float32))\n",
      "Average Return:  10.0\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "policy = random_py_policy.RandomPyPolicy(time_step_spec=env.time_step_spec(),\n",
    "                                         action_spec=env.action_spec())\n",
    "replay_buffer = []\n",
    "metric = py_metrics.AverageReturnMetric()\n",
    "observers = [replay_buffer.append, metric]\n",
    "driver = py_driver.PyDriver(env,\n",
    "                            policy,\n",
    "                            observers,\n",
    "                            max_steps=20,\n",
    "                            max_episodes=1)\n",
    "\n",
    "initial_time_step = env.reset()\n",
    "final_time_step, _ = driver.run(initial_time_step)\n",
    "\n",
    "print('Replay Buffer:')\n",
    "for traj in replay_buffer:\n",
    "    print(traj)\n",
    "\n",
    "print('Average Return: ', metric.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Drivers\n",
    "\n",
    "我们在TensorFlow中也有一些drivers，它们的功能与Python中的drivers类似，但是使用TF环境、TF策略、TF观察者等。我们目前有两个TensorFlow的driver:`DynamicStepDriver`，它在给定数量的(有效的)环境步骤之后终止;`DynamicEpisodeDriver`它在给定数量的事件之后终止。让我们来看一个DynamicEpisodeDriver的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_time_step TimeStep(step_type=<tf.Tensor: id=2194, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: id=2195, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: id=2196, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: id=2197, shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.04311429,  0.02406621,  0.02612159, -0.01080346]],\n",
      "      dtype=float32)>)\n",
      "Number of Steps:  53\n",
      "Number of Episodes:  2\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "tf_policy = random_tf_policy.RandomTFPolicy(action_spec=tf_env.action_spec(),\n",
    "                                            time_step_spec=tf_env.time_step_spec())\n",
    "\n",
    "\n",
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "observers = [num_episodes, env_steps]\n",
    "driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env, tf_policy, observers, num_episodes=2)\n",
    "\n",
    "# Initial driver.run will reset the environment and initialize the policy.\n",
    "final_time_step, policy_state = driver.run()\n",
    "\n",
    "print('final_time_step', final_time_step)\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_time_step TimeStep(step_type=<tf.Tensor: id=3845, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: id=3846, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: id=3847, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: id=3848, shape=(1, 4), dtype=float32, numpy=\n",
      "array([[ 0.04527216,  0.03846889, -0.02772892, -0.00087081]],\n",
      "      dtype=float32)>)\n",
      "Number of Steps:  93\n",
      "Number of Episodes:  4\n"
     ]
    }
   ],
   "source": [
    "# Continue running from previous state\n",
    "final_time_step, _ = driver.run(final_time_step, policy_state)\n",
    "\n",
    "print('final_time_step', final_time_step)\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
