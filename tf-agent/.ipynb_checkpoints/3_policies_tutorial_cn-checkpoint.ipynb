{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Started\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/3_policies_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/3_policies_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tfp-nightly in /Users/liangqy/.local/lib/python3.7/site-packages (0.9.0.dev20191129)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/liangqy/anaconda3/lib/python3.7/site-packages (from tfp-nightly) (1.17.3)\n",
      "Requirement already satisfied: decorator in /Users/liangqy/anaconda3/lib/python3.7/site-packages (from tfp-nightly) (4.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/liangqy/anaconda3/lib/python3.7/site-packages (from tfp-nightly) (1.11.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.2 in /Users/liangqy/anaconda3/lib/python3.7/site-packages (from tfp-nightly) (1.2.2)\n",
      "Requirement already satisfied: gast>=0.2 in /Users/liangqy/anaconda3/lib/python3.7/site-packages (from tfp-nightly) (0.2.2)\n",
      "Requirement already satisfied: tf-agents-nightly in /Users/liangqy/.local/lib/python3.7/site-packages (0.2.0.dev20191129)\n",
      "Requirement already satisfied: tfp-nightly in /Users/liangqy/.local/lib/python3.7/site-packages (from tf-agents-nightly) (0.9.0.dev20191129)\n",
      "Requirement already satisfied: gin-config==0.1.3 in /Users/liangqy/.local/lib/python3.7/site-packages (from tf-agents-nightly) (0.1.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/liangqy/anaconda3/lib/python3.7/site-packages (from tf-agents-nightly) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/liangqy/anaconda3/lib/python3.7/site-packages (from tf-agents-nightly) (1.17.3)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /Users/liangqy/anaconda3/lib/python3.7/site-packages (from tf-agents-nightly) (0.8.1)\n",
      "Requirement already satisfied: decorator in /Users/liangqy/anaconda3/lib/python3.7/site-packages (from tfp-nightly->tf-agents-nightly) (4.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.2 in /Users/liangqy/anaconda3/lib/python3.7/site-packages (from tfp-nightly->tf-agents-nightly) (1.2.2)\n",
      "Requirement already satisfied: gast>=0.2 in /Users/liangqy/anaconda3/lib/python3.7/site-packages (from tfp-nightly->tf-agents-nightly) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Note: If you haven't installed tf-agents yet, run:\n",
    "try:\n",
    "    %%tensorflow_version 2.x\n",
    "except:\n",
    "    pass\n",
    "!pip install tfp-nightly\n",
    "!pip install tf-agents-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import network\n",
    "\n",
    "from tf_agents.policies import py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import actor_policy\n",
    "from tf_agents.policies import q_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简介\n",
    "\n",
    "在强化学习中，策略将来自环境的观察映射到操作或操作上的分布。 在TF-Agents中，来自环境的观察包含一个命名为TimeStep的元组`TimeStep('step_type', 'discount', 'reward', 'observation')`，策略将时间步骤映射到操作或操作之上的分布。大多数策略使用  `timestep.observation`, 有一些策略使用 `timestep.step_type` (例如 在有状态策略的事件开始时重置状态), 但是 `timestep.discount` 和 `timestep.reward` 常常被忽略。\n",
    "\n",
    "策略以如下方式与tf - agent中的其他组件相关：大多数策略都有一个神经网络来计算动作和/或动作在TimeSteps上的分布。代理可以包含一个或多个用于不同目的的策略，例如，正在为部署而训练的主策略和用于数据收集的嘈杂策略。可以保存/恢复策略，可以独立地用于代理的数据收集、评估等。\n",
    "\n",
    "有些策略更容易用Tensorflow编写(例如使用神经网络的策略)，而另一些策略更容易用Python编写(例如遵循操作脚本)。所以在TF代理中，我们同时允许Python和Tensorflow策略。此外，用TensorFlow编写的策略可能必须在Python环境中使用，反之亦然，例如，TensorFlow策略用于训练，但稍后将部署到生产Python环境中。为了简化这一点，我们提供了用于在python和TensorFlow策略之间进行转换的包装器。\n",
    "\n",
    "另一类有趣的策略是策略包装器，它以某种方式修改给定的策略，例如添加某种特定类型的噪声，对随机策略采用贪心算法或$\\epsilon$贪心算法的修改，将多个策略随机混合等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Python的策略\n",
    "\n",
    "使用Python中策略的接口被定义在`policies/py_policy.Base`中，主要的函数如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(object):\n",
    "    @abc.abstractmethod\n",
    "    def __init__(self, time_step_spec, action_spec, policy_state_spec=()):\n",
    "        self._time_step_spec = time_step_spec\n",
    "        self._action_spec = action_spec\n",
    "        self._policy_state_spec = policy_state_spec\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def reset(self, policy_state=()):\n",
    "        # return initial_policy_state.\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def action(self, time_step, policy_state=()):\n",
    "        # return a PolicyStep(action, state, info) named tuple.\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def distribution(self, time_step, policy_state=()):\n",
    "        # Not implemented in python, only for TF policies.\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def update(self, policy):\n",
    "        # update self to be similar to the input `policy`.\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def copy(self):\n",
    "        # return a copy of self.\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def time_step_spec(self):\n",
    "        return self._time_step_spec\n",
    "\n",
    "    @property\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    @property\n",
    "    def policy_state_spec(self):\n",
    "        return self._policy_state_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最重要的方法是`action(time_step)` ，它将包含一个从环境中观察得到的`time_step` 映射为PolicyStep的元组，此元组包含以下的属性：\n",
    "\n",
    "*  `action`: 应用于环境中的动作\n",
    "*  `state`: 被应用于下一次动作调用时策略的状态 (例如 RNN 的状态) \n",
    "*  `info`: 可选的次要信息，如动作的对数概率\n",
    "\n",
    " `time_step_spec` 和 `action_spec` 是输入时间步长和输出操作的规范。策略还有一个“reset”函数，通常用于在有状态策略中重置状态。' copy '函数返回' self '的副本，' update(new_policy) '函数将' self '更新为' new_policy '。\n",
    "\n",
    "现在，让我们看几个python策略的例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例1: 随机的Python策略\n",
    "\n",
    " `RandomPyPolicy`是一个简单的`PyPolicy`，它可以在已知离散/连续的动作空间内生成随机的动作，输入的 `time_step`被忽略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyStep(action=array([-3, -6], dtype=int32), state=(), info=())\n",
      "PolicyStep(action=array([-2, -8], dtype=int32), state=(), info=())\n"
     ]
    }
   ],
   "source": [
    "action_spec = array_spec.BoundedArraySpec((2,), np.int32, -10, 10)\n",
    "my_random_py_policy = random_py_policy.RandomPyPolicy(time_step_spec=None,\n",
    "    action_spec=action_spec)\n",
    "time_step = None\n",
    "action_step = my_random_py_policy.action(time_step)\n",
    "print(action_step)\n",
    "action_step = my_random_py_policy.action(time_step)\n",
    "print(action_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例 2: Python脚本化策略\n",
    "\n",
    "脚本化策略将操作的脚本表示为' (num_repeat, action) '元组的列表。每次调用“action”函数时，它都会从列表中返回下一个操作，直到完成指定的重复次数，然后移动到列表中的下一个操作。可以调用“reset”方法从列表的开头开始执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing scripted policy...\n",
      "PolicyStep(action=array([5, 2], dtype=int32), state=[0, 1], info=())\n",
      "PolicyStep(action=array([1, 2], dtype=int32), state=[2, 1], info=())\n",
      "PolicyStep(action=array([1, 2], dtype=int32), state=[2, 2], info=())\n",
      "Resetting my_scripted_py_policy...\n",
      "PolicyStep(action=array([5, 2], dtype=int32), state=[0, 1], info=())\n"
     ]
    }
   ],
   "source": [
    "action_spec = array_spec.BoundedArraySpec((2,), np.int32, -10, 10)\n",
    "action_script = [(1, np.array([5, 2], dtype=np.int32)), \n",
    "                 (0, np.array([0, 0], dtype=np.int32)), # Setting `num_repeates` to 0 will skip this action.\n",
    "                 (2, np.array([1, 2], dtype=np.int32)), \n",
    "                 (1, np.array([3, 4], dtype=np.int32))]\n",
    "\n",
    "my_scripted_py_policy = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=None, action_spec=action_spec, action_script=action_script)\n",
    "\n",
    "policy_state = my_scripted_py_policy.get_initial_state()\n",
    "time_step = None\n",
    "print('Executing scripted policy...')\n",
    "action_step = my_scripted_py_policy.action(time_step, policy_state)\n",
    "print(action_step)\n",
    "action_step= my_scripted_py_policy.action(time_step, action_step.state)\n",
    "print(action_step)\n",
    "action_step = my_scripted_py_policy.action(time_step, action_step.state)\n",
    "print(action_step)\n",
    "\n",
    "print('Resetting my_scripted_py_policy...')\n",
    "policy_state = my_scripted_py_policy.get_initial_state()\n",
    "action_step = my_scripted_py_policy.action(time_step, policy_state)\n",
    "print(action_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用TensorFlow的策略\n",
    "\n",
    "使用TensorFlow的策略和使用Python的策略有着同样的接口，下面是几个例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例 1: 随机的 TF 策略\n",
    "\n",
    "一个随机的TFPolicy 可以根据给定的离散/连续的`action_spec`生成一个随机的动作. 输入的 `time_step`会被忽略。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:\n",
      "tf.Tensor([-0.14009571 -0.49590063], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    (2,), tf.float32, minimum=-1, maximum=3)\n",
    "input_tensor_spec = tensor_spec.TensorSpec((2,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "\n",
    "my_random_tf_policy = random_tf_policy.RandomTFPolicy(\n",
    "    action_spec=action_spec, time_step_spec=time_step_spec)\n",
    "observation = tf.ones(time_step_spec.observation.shape)\n",
    "time_step = ts.restart(observation)\n",
    "action_step = my_random_tf_policy.action(time_step)\n",
    "\n",
    "print('Action:')\n",
    "print(action_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例 2: Actor Policy\n",
    "\n",
    "可以使用将'time_steps'映射到操作的网络或将' time_steps '映射到操作上的发行版的网络来创建actor策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 action network\n",
    "\n",
    "让我们这样来定义一个网络:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNet(network.Network):\n",
    "    def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "        super(ActionNet, self).__init__(input_tensor_spec=input_tensor_spec,\n",
    "                                        state_spec=(),\n",
    "                                        name='ActionNet')\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "        self._layers = [\n",
    "            tf.keras.layers.Dense(action_spec.shape.num_elements(),\n",
    "                                  activation=tf.nn.tanh),\n",
    "        ]\n",
    "\n",
    "    def call(self, observations, step_type, network_state):\n",
    "        del step_type\n",
    "\n",
    "        output = tf.cast(observations, dtype=tf.float32)\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "        actions = tf.reshape(output,\n",
    "                             [-1] + self._output_tensor_spec.shape.as_list())\n",
    "\n",
    "        # Scale and shift actions to the correct range if necessary.\n",
    "        return actions, network_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在TensorFlow中，大多数网络层都是为批处理操作设计的，因此我们希望对输入time_steps进行批处理，同时对网络的输出也进行批处理。网络还负责生成给定action_spec的正确范围内的操作。这通常是通过如下方法来完成的:首先激活最后一层的tanh，生成[- 1,1]中的动作，然后将其作为输入action_spec缩放并移动到正确的范围(更多请查看 `tf_agents/agents/ddpg/networks.actor_network()`).\n",
    "\n",
    "现在，我们使用上面的网络创建一个actor策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_spec = tensor_spec.TensorSpec((4,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "action_spec = tensor_spec.BoundedTensorSpec((3,),\n",
    "                                            tf.float32,\n",
    "                                            minimum=-1,\n",
    "                                            maximum=1)\n",
    "\n",
    "action_net = ActionNet(input_tensor_spec, action_spec)\n",
    "\n",
    "my_actor_policy = actor_policy.ActorPolicy(\n",
    "    time_step_spec=time_step_spec,\n",
    "    action_spec=action_spec,\n",
    "    actor_network=action_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将它应用到任何批次遵循time_step_spe的time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:\n",
      "tf.Tensor(\n",
      "[[-0.8051854  0.6902697  0.5453182]\n",
      " [-0.8051854  0.6902697  0.5453182]], shape=(2, 3), dtype=float32)\n",
      "Action distribution:\n",
      "tfp.distributions.Deterministic(\"Deterministic\", batch_shape=[2, 3], event_shape=[], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "observations = tf.ones([2] + time_step_spec.observation.shape.as_list())\n",
    "\n",
    "time_step = ts.restart(observations, batch_size)\n",
    "\n",
    "action_step = my_actor_policy.action(time_step)\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "\n",
    "distribution_step = my_actor_policy.distribution(time_step)\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中，我们使用产生一个动作张量的动作网络来创建策略。在本例中，`policy.distribution(time_step)`是围绕`policy.action(time_step)`输出的确定性(增量)分布。生成随机策略的一种方法是将actor策略包装在一个策略包装器中，该包装器将向操作添加噪声。另一种方法是使用动作分布网络（ action distribution network）而不是操作网络来创建actor策略，如下所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用一个动作分布网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:\n",
      "tf.Tensor(\n",
      "[[-0.2039079   0.11148027  0.30938396]\n",
      " [-0.5325132  -0.83604133 -1.        ]], shape=(2, 3), dtype=float32)\n",
      "Action distribution:\n",
      "tfp.distributions.Normal(\"ActionNet_Normal\", batch_shape=[2, 3], event_shape=[], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class ActionDistributionNet(ActionNet):\n",
    "    def call(self, observations, step_type, network_state):\n",
    "        action_means, network_state = super(ActionDistributionNet,\n",
    "                                            self).call(observations, step_type,\n",
    "                                                       network_state)\n",
    "\n",
    "        action_std = tf.ones_like(action_means)\n",
    "        return tfp.distributions.Normal(action_means,\n",
    "                                        action_std), network_state\n",
    "\n",
    "\n",
    "action_distribution_net = ActionDistributionNet(input_tensor_spec, action_spec)\n",
    "\n",
    "my_actor_policy = actor_policy.ActorPolicy(\n",
    "    time_step_spec=time_step_spec,\n",
    "    action_spec=action_spec,\n",
    "    actor_network=action_distribution_net)\n",
    "\n",
    "action_step = my_actor_policy.action(time_step)\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "distribution_step = my_actor_policy.distribution(time_step)\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，在上面，动作被裁剪到给定动作规范的范围[- 1,1]。这是因为ActorPolicy总 clip的构造函数参数默认为True。将此设置为false将返回网络生成的未剪切操作。\n",
    "\n",
    "随机策略可以转换为确定性策略，例如，使用一个GreedyPolicy包装器，该包装器选择`stochastic_policy.distribution().mode()` 作为其操作, 而围绕这个贪婪操作的一个确定性/增量分布作为 `distribution()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例 3: Q Policy\n",
    "\n",
    "Q策略用于像DQN这样的代理，它基于Q网络，该网络预测每个离散操作的Q值。对于给定的时间步长，Q策略中的操作分布是一个使用Q值作为logits创建的分类分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:\n",
      "tf.Tensor(\n",
      "[[-1]\n",
      " [ 0]], shape=(2, 1), dtype=int32)\n",
      "Action distribution:\n",
      "tfp.distributions.ShiftedCategorical(\"ShiftedCategorical\", batch_shape=[2, 1], event_shape=[], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "input_tensor_spec = tensor_spec.TensorSpec((4, ), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "action_spec = tensor_spec.BoundedTensorSpec((1, ),\n",
    "                                            tf.int32,\n",
    "                                            minimum=-1,\n",
    "                                            maximum=1)\n",
    "num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "\n",
    "\n",
    "class QNetwork(network.Network):\n",
    "    def __init__(self,\n",
    "                 input_tensor_spec,\n",
    "                 action_spec,\n",
    "                 num_actions=2,\n",
    "                 name=None):\n",
    "        super(QNetwork, self).__init__(input_tensor_spec=input_tensor_spec,\n",
    "                                       state_spec=(),\n",
    "                                       name=name)\n",
    "        self._layers.append(tf.keras.layers.Dense(num_actions))\n",
    "\n",
    "    def call(self, inputs, step_type=None, network_state=()):\n",
    "        del step_type\n",
    "        inputs = tf.cast(inputs, tf.float32)\n",
    "        for layer in self.layers:\n",
    "            inputs = layer(inputs)\n",
    "        return inputs, network_state\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "observation = tf.ones([batch_size] +\n",
    "                      time_step_spec.observation.shape.as_list())\n",
    "time_steps = ts.restart(observation, batch_size=batch_size)\n",
    "\n",
    "my_q_network = QNetwork(input_tensor_spec=input_tensor_spec,\n",
    "                        action_spec=action_spec)\n",
    "my_q_policy = q_policy.QPolicy(time_step_spec,\n",
    "                               action_spec,\n",
    "                               q_network=my_q_network)\n",
    "action_step = my_q_policy.action(time_steps)\n",
    "distribution_step = my_q_policy.distribution(time_steps)\n",
    "\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略包装器（Policy Wrappers）\n",
    "\n",
    "策略包装器可用于包装和修改给定的策略，例如添加噪声。策略包装器是策略的子类(Python/TensorFlow)，因此可以像其他策略一样使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例: 贪心策略（Greedy Policy）\n",
    "\n",
    "\n",
    "贪婪的包装器可以用来包装任何实现`distribution()`的TensorFlow策略。`GreedyPolicy.action()`将返回`wrapped_policy.distribution().mode()` ，而且 `GreedyPolicy.distribution()` i是一个围绕`GreedyPolicy.action()`的确定性/增量分布:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:\n",
      "tf.Tensor(\n",
      "[[-1]\n",
      " [-1]], shape=(2, 1), dtype=int32)\n",
      "Action distribution:\n",
      "tfp.distributions.DeterministicWithLogProb(\"Deterministic\", batch_shape=[2, 1], event_shape=[], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "my_greedy_policy = greedy_policy.GreedyPolicy(my_q_policy)\n",
    "\n",
    "action_step = my_greedy_policy.action(time_steps)\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "\n",
    "distribution_step = my_greedy_policy.distribution(time_steps)\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
